{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roE1ytfsp3DP"
      },
      "outputs": [],
      "source": [
        "# PHASE 1 - DATA COLLECTION AND EXPLORATORY DATA ANALYSIS (EDA)\n",
        "\n",
        "# Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and Preprocess Data\n",
        "# Upload the employee_data.csv file from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "# Make sure to adjust the path to where your file is located\n",
        "df = pd.read_csv('/content/drive/MyDrive/employee_data.csv')\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "04gRkTk9qG4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 (continued): Data Inspection and Cleaning\n",
        "# Check dataset information\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nDuplicate Rows:\")\n",
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()  # Remove rows with missing values\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"\\nCleaned Dataset Shape: {df.shape}\")\n",
        "print(\"Data cleaning completed successfully!\")"
      ],
      "metadata": {
        "id": "TO4Y2jRqqLnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Exploratory Data Analysis (EDA)\n",
        "# Calculate Descriptive Statistics\n",
        "print(\"DESCRIPTIVE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = ['Age', 'Salary', 'YearsAtCompany', 'PerformanceScore']\n",
        "\n",
        "for col in numerical_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Mean: {df[col].mean():.2f}\")\n",
        "    print(f\"  Median: {df[col].median():.2f}\")\n",
        "    print(f\"  Mode: {df[col].mode().values[0] if len(df[col].mode()) > 0 else 'N/A'}\")\n",
        "    print(f\"  Variance: {df[col].var():.2f}\")\n",
        "    print(f\"  Standard Deviation: {df[col].std():.2f}\")\n",
        "    print(f\"  Min: {df[col].min():.2f}\")\n",
        "    print(f\"  Max: {df[col].max():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nOverall Statistical Summary:\")\n",
        "print(df[numerical_cols].describe())"
      ],
      "metadata": {
        "id": "HMJ67ajbqQw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 (continued): Visualizations - Pairplot\n",
        "print(\"Creating Pairplot to explore relationships between features...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "pairplot = sns.pairplot(df, hue='Attrition', vars=numerical_cols, diag_kind='kde')\n",
        "pairplot.fig.suptitle('Pairplot: Feature Relationships by Attrition', y=1.02, fontsize=16)\n",
        "plt.show()\n",
        "print(\"Pairplot created successfully!\")"
      ],
      "metadata": {
        "id": "uRKTjbNMqWrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 (continued): Correlation Heatmap and Outliers\n",
        "print(\"Creating Correlation Heatmap...\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f',\n",
        "            square=True, linewidths=1)\n",
        "plt.title('Correlation Heatmap of Numerical Features', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nIdentifying Outliers using Boxplots...\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    ax = axes[idx//2, idx%2]\n",
        "    ax.boxplot(df[col])\n",
        "    ax.set_title(f'Boxplot: {col}', fontweight='bold')\n",
        "    ax.set_ylabel('Values')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Visualizations completed successfully!\")"
      ],
      "metadata": {
        "id": "dwgVUk7uqaM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Probability and Statistical Analysis\n",
        "# Calculate probability of employee leaving\n",
        "print(\"PROBABILITY AND STATISTICAL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate probability of attrition\n",
        "attrition_counts = df['Attrition'].value_counts()\n",
        "print(\"\\nAttrition Distribution:\")\n",
        "print(attrition_counts)\n",
        "\n",
        "# Calculate overall probability\n",
        "total_employees = len(df)\n",
        "attrition_yes = attrition_counts.get('Yes', 0)\n",
        "attrition_no = attrition_counts.get('No', 0)\n",
        "\n",
        "prob_attrition = attrition_yes / total_employees\n",
        "prob_retention = attrition_no / total_employees\n",
        "\n",
        "print(f\"\\nProbability of Employee Leaving (Attrition): {prob_attrition:.4f} ({prob_attrition*100:.2f}%)\")\n",
        "print(f\"Probability of Employee Staying (Retention): {prob_retention:.4f} ({prob_retention*100:.2f}%)\")\n",
        "\n",
        "# Probability based on performance score\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nProbability of Attrition by Performance Score:\")\n",
        "for score in sorted(df['PerformanceScore'].unique()):\n",
        "    subset = df[df['PerformanceScore'] == score]\n",
        "    attrition_at_score = subset[subset['Attrition'] == 'Yes'].shape[0]\n",
        "    prob = attrition_at_score / len(subset) if len(subset) > 0 else 0\n",
        "    print(f\"  Performance Score {score}: {prob:.4f} ({prob*100:.2f}%)\")\n",
        "\n",
        "# Probability based on department\n",
        "print(\"\\nProbability of Attrition by Department:\")\n",
        "for dept in df['Department'].unique():\n",
        "    subset = df[df['Department'] == dept]\n",
        "    attrition_at_dept = subset[subset['Attrition'] == 'Yes'].shape[0]\n",
        "    prob = attrition_at_dept / len(subset) if len(subset) > 0 else 0\n",
        "    print(f\"  {dept}: {prob:.4f} ({prob*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "VvdhZNNLqeRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 (continued): Bayes' Theorem\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nBAYES' THEOREM - P(Attrition|Performance Score)\\n\")\n",
        "\n",
        "# Example: Calculate P(Attrition|PerformanceScore >= 85)\n",
        "# P(A|B) = P(B|A) * P(A) / P(B)\n",
        "\n",
        "# Define high performance (>= 85)\n",
        "high_perf = df[df['PerformanceScore'] >= 85]\n",
        "low_perf = df[df['PerformanceScore'] < 85]\n",
        "\n",
        "# P(Attrition)\n",
        "P_attrition = len(df[df['Attrition'] == 'Yes']) / len(df)\n",
        "\n",
        "# P(High Performance|Attrition)\n",
        "attrition_df = df[df['Attrition'] == 'Yes']\n",
        "P_high_given_attrition = len(attrition_df[attrition_df['PerformanceScore'] >= 85]) / len(attrition_df)\n",
        "\n",
        "# P(High Performance)\n",
        "P_high_perf = len(high_perf) / len(df)\n",
        "\n",
        "# Apply Bayes' Theorem\n",
        "if P_high_perf > 0:\n",
        "    P_attrition_given_high = (P_high_given_attrition * P_attrition) / P_high_perf\n",
        "else:\n",
        "    P_attrition_given_high = 0\n",
        "\n",
        "print(f\"P(Attrition) = {P_attrition:.4f}\")\n",
        "print(f\"P(High Performance | Attrition) = {P_high_given_attrition:.4f}\")\n",
        "print(f\"P(High Performance) = {P_high_perf:.4f}\")\n",
        "print(f\"\\nP(Attrition | High Performance >= 85) = {P_attrition_given_high:.4f}\")\n",
        "print(f\"\\nInterpretation: The probability of attrition given high performance is {P_attrition_given_high*100:.2f}%\")"
      ],
      "metadata": {
        "id": "fA00TOaiqi9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 (continued): Hypothesis Testing\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nHYPOTHESIS TESTING - ANOVA Test\")\n",
        "print(\"H0: Mean performance score is the same across all departments\")\n",
        "print(\"H1: Mean performance score differs across departments\\n\")\n",
        "\n",
        "# Group data by department\n",
        "departments = df.groupby('Department')['PerformanceScore'].apply(list)\n",
        "dept_groups = [df[df['Department'] == dept]['PerformanceScore'].values for dept in df['Department'].unique()]\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(*dept_groups)\n",
        "\n",
        "print(f\"F-Statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "print(f\"\\nSignificance Level (alpha): 0.05\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"\\nResult: REJECT the null hypothesis\")\n",
        "    print(\"Conclusion: There is a significant difference in mean performance scores across departments.\")\n",
        "else:\n",
        "    print(\"\\nResult: FAIL TO REJECT the null hypothesis\")\n",
        "    print(\"Conclusion: There is no significant difference in mean performance scores across departments.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nPhase 1 Complete!\\n\")"
      ],
      "metadata": {
        "id": "wN_aBm_9qmxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 2 - PREDICTIVE MODELING\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 2: PREDICTIVE MODELING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Step 4: Feature Engineering and Encoding\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
        "\n",
        "print(\"\\nStep 4: Feature Engineering and Encoding\\n\")\n",
        "\n",
        "# Create a copy of the dataframe\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Label Encoding for categorical variables\n",
        "le = LabelEncoder()\n",
        "df_processed['Attrition_Encoded'] = le.fit_transform(df_processed['Attrition'])\n",
        "df_processed['Department_Encoded'] = le.fit_transform(df_processed['Department'])\n",
        "\n",
        "print(\"Encoding Completed:\")\n",
        "print(f\"  - Attrition: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "# Min-Max Scaling for numerical features\n",
        "scaler = MinMaxScaler()\n",
        "numerical_features = ['Age', 'Salary', 'YearsAtCompany', 'PerformanceScore']\n",
        "df_processed[numerical_features] = scaler.fit_transform(df_processed[numerical_features])\n",
        "\n",
        "print(f\"\\nMin-Max Scaling applied to: {numerical_features}\")\n",
        "print(f\"\\nProcessed Dataset Shape: {df_processed.shape}\")\n",
        "print(\"\\nFirst 5 rows of processed data:\")\n",
        "df_processed.head()"
      ],
      "metadata": {
        "id": "5OFg7dY7qq9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Employee Attrition Prediction Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Step 5: Employee Attrition Prediction Model\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_processed[['Age', 'Salary', 'YearsAtCompany', 'PerformanceScore', 'Department_Encoded']]\n",
        "y = df_processed['Attrition_Encoded']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "print(\"\\nTraining Random Forest Classifier...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Attrition', 'Attrition']))"
      ],
      "metadata": {
        "id": "4EjM7DrcqvLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 (continued): Confusion Matrix Visualization\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Attrition', 'Attrition'],\n",
        "            yticklabels=['No Attrition', 'Attrition'])\n",
        "plt.title('Confusion Matrix - Attrition Prediction', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nStep 5 Complete!\")"
      ],
      "metadata": {
        "id": "X3m-HVOeqylh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Employee Performance Prediction Model (Regression)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Step 6: Employee Performance Prediction Model\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare features and target for regression\n",
        "X_perf = df_processed[['Age', 'Salary', 'YearsAtCompany', 'Department_Encoded']]\n",
        "y_perf = df_processed['PerformanceScore']\n",
        "\n",
        "# Split the data\n",
        "X_train_perf, X_test_perf, y_train_perf, y_test_perf = train_test_split(\n",
        "    X_perf, y_perf, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train_perf.shape}\")\n",
        "print(f\"Testing set size: {X_test_perf.shape}\")\n",
        "\n",
        "# Train Linear Regression Model\n",
        "print(\"\\nTraining Linear Regression Model...\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_perf, y_train_perf)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_perf = lr_model.predict(X_test_perf)\n",
        "\n",
        "# Evaluate the model\n",
        "r2 = r2_score(y_test_perf, y_pred_perf)\n",
        "mse = mean_squared_error(y_test_perf, y_pred_perf)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_perf, y_pred_perf)\n",
        "\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"  R² Score: {r2:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse:.6f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae:.6f}\")"
      ],
      "metadata": {
        "id": "6XbHnnO6q3Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 (continued): Visualize Predicted vs Actual Performance\n",
        "print(\"\\nVisualizing Predicted vs Actual Performance Scores...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_perf, y_pred_perf, alpha=0.6, edgecolors='k')\n",
        "plt.plot([y_test_perf.min(), y_test_perf.max()],\n",
        "         [y_test_perf.min(), y_test_perf.max()],\n",
        "         'r--', lw=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual Performance Score', fontsize=12)\n",
        "plt.ylabel('Predicted Performance Score', fontsize=12)\n",
        "plt.title('Predicted vs Actual Performance Scores', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Phase 2 Complete!\\n\")"
      ],
      "metadata": {
        "id": "1-76jD8Kq8Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 3 - DEEP LEARNING MODELS\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 3: DEEP LEARNING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Step 7: Deep Learning for Employee Performance Prediction\n",
        "print(\"\\nStep 7: Deep Learning for Performance Prediction\\n\")\n",
        "\n",
        "# Prepare data (using the same split as before)\n",
        "X_train_dl = X_train_perf.values\n",
        "X_test_dl = X_test_perf.values\n",
        "y_train_dl = y_train_perf.values\n",
        "y_test_dl = y_test_perf.values\n",
        "\n",
        "print(f\"Training data shape: {X_train_dl.shape}\")\n",
        "print(f\"Testing data shape: {X_test_dl.shape}\")\n",
        "\n",
        "# Build Feedforward Neural Network\n",
        "print(\"\\nBuilding Feedforward Neural Network...\")\n",
        "model_performance = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_dl.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_performance.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "model_performance.summary()"
      ],
      "metadata": {
        "id": "D-1srsr6rBWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 (continued): Train the model\n",
        "print(\"\\nTraining the Neural Network...\")\n",
        "history = model_performance.fit(\n",
        "    X_train_dl, y_train_dl,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_mae = model_performance.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Test Loss (MSE): {test_loss:.6f}\")\n",
        "print(f\"  Test MAE: {test_mae:.6f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dl = model_performance.predict(X_test_dl, verbose=0)\n",
        "r2_dl = r2_score(y_test_dl, y_pred_dl)\n",
        "print(f\"  R² Score: {r2_dl:.4f}\")"
      ],
      "metadata": {
        "id": "WZBJunQqrFda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Employee Attrition Analysis with Deep Learning\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Step 8: Employee Attrition Prediction using Deep Learning\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare data for classification\n",
        "X_train_attr = X_train.values\n",
        "X_test_attr = X_test.values\n",
        "y_train_attr = y_train.values\n",
        "y_test_attr = y_test.values\n",
        "\n",
        "print(f\"Training data shape: {X_train_attr.shape}\")\n",
        "print(f\"Testing data shape: {X_test_attr.shape}\")\n",
        "\n",
        "# Build Neural Network for Classification\n",
        "print(\"\\nBuilding Neural Network for Attrition Prediction...\")\n",
        "model_attrition = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X_train_attr.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_attrition.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "model_attrition.summary()"
      ],
      "metadata": {
        "id": "9QJ8VxburKAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8 (continued): Train and Evaluate\n",
        "print(\"\\nTraining the Attrition Prediction Model...\")\n",
        "history_attr = model_attrition.fit(\n",
        "    X_train_attr, y_train_attr,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = model_attrition.evaluate(X_test_attr, y_test_attr, verbose=0)\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Test Loss: {test_results[0]:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_results[1]:.4f}\")\n",
        "print(f\"  Test Precision: {test_results[2]:.4f}\")\n",
        "print(f\"  Test Recall: {test_results[3]:.4f}\")\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1_dl = 2 * (test_results[2] * test_results[3]) / (test_results[2] + test_results[3]) if (test_results[2] + test_results[3]) > 0 else 0\n",
        "print(f\"  F1-Score: {f1_dl:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Phase 3 Complete!\\n\")"
      ],
      "metadata": {
        "id": "SdaSdH-qrOQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 4 - REPORTING AND INSIGHTS\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 4: REPORTING AND INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Step 9: Insights and Recommendations\n",
        "print(\"\\nStep 9: Insights and Recommendations\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nKEY FINDINGS:\\n\")\n",
        "print(\"1. EMPLOYEE PERFORMANCE FACTORS:\")\n",
        "\n",
        "# Analyze feature importance from Random Forest\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': ['Age', 'Salary', 'YearsAtCompany', 'PerformanceScore', 'Department'],\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n   Top factors contributing to employee performance:\")\n",
        "for idx, row in feature_importance.iterrows():\n",
        "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
        "\n",
        "print(\"\\n2. HIGH-RISK DEPARTMENTS FOR ATTRITION:\")\n",
        "attrition_by_dept = df.groupby('Department')['Attrition'].apply(\n",
        "    lambda x: (x == 'Yes').sum() / len(x) * 100\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "for dept, rate in attrition_by_dept.items():\n",
        "    risk_level = \"HIGH\" if rate > 40 else \"MEDIUM\" if rate > 25 else \"LOW\"\n",
        "    print(f\"   - {dept}: {rate:.2f}% attrition rate [{risk_level} RISK]\")\n",
        "\n",
        "print(\"\\n3. PERFORMANCE SCORE DISTRIBUTION:\")\n",
        "perf_stats = df['PerformanceScore'].describe()\n",
        "print(f\"   - Average Performance Score: {perf_stats['mean']:.2f}\")\n",
        "print(f\"   - High Performers (>= 85): {len(df[df['PerformanceScore'] >= 85])} employees ({len(df[df['PerformanceScore'] >= 85])/len(df)*100:.1f}%)\")\n",
        "print(f\"   - Low Performers (< 70): {len(df[df['PerformanceScore'] < 70])} employees ({len(df[df['PerformanceScore'] < 70])/len(df)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "YS7wHXSPrUO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9 (continued): Recommendations\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nRECOMMENDATIONS FOR IMPROVEMENT:\\n\")\n",
        "\n",
        "print(\"1. Department-wise Performance Improvement Plans:\")\n",
        "for dept in df['Department'].unique():\n",
        "    dept_data = df[df['Department'] == dept]\n",
        "    avg_perf = dept_data['PerformanceScore'].mean()\n",
        "    print(f\"   - {dept}: Average performance {avg_perf:.2f}\")\n",
        "    if avg_perf < 80:\n",
        "        print(f\"     → Implement targeted training and development programs\")\n",
        "\n",
        "print(\"\\n2. Targeted Employee Engagement Programs:\")\n",
        "print(\"   - Focus on high-risk departments with attrition rates > 30%\")\n",
        "print(\"   - Develop mentorship programs for employees with < 5 years experience\")\n",
        "print(\"   - Create career development paths for high performers\")\n",
        "\n",
        "print(\"\\n3. Retention Strategies:\")\n",
        "print(\"   - Regular performance reviews and feedback sessions\")\n",
        "print(\"   - Competitive compensation packages for high performers\")\n",
        "print(\"   - Work-life balance initiatives\")\n",
        "print(\"   - Recognition and rewards programs\")\n",
        "\n",
        "print(\"\\n4. Predictive Model Integration:\")\n",
        "print(f\"   - Use attrition prediction model (Accuracy: {accuracy:.2%}) to identify at-risk employees\")\n",
        "print(f\"   - Use performance prediction model (R²: {r2:.2f}) for workforce planning\")\n",
        "print(\"   - Implement early warning system for potential attrition\")"
      ],
      "metadata": {
        "id": "I9heGeR5rb1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Data Visualization and Reporting\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Step 10: Data Visualization and Reporting\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create comprehensive visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Line Plot - Performance Trends by Years at Company\n",
        "ax1 = axes[0, 0]\n",
        "perf_by_years = df.groupby('YearsAtCompany')['PerformanceScore'].mean().sort_index()\n",
        "ax1.plot(perf_by_years.index, perf_by_years.values, marker='o', linewidth=2, markersize=8)\n",
        "ax1.set_title('Performance Score Trends by Years at Company', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Years at Company')\n",
        "ax1.set_ylabel('Average Performance Score')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Bar Chart - Attrition by Department\n",
        "ax2 = axes[0, 1]\n",
        "attrition_counts = df.groupby(['Department', 'Attrition']).size().unstack(fill_value=0)\n",
        "attrition_counts.plot(kind='bar', ax=ax2, color=['#2ecc71', '#e74c3c'])\n",
        "ax2.set_title('Attrition Distribution by Department', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Department')\n",
        "ax2.set_ylabel('Number of Employees')\n",
        "ax2.legend(['No Attrition', 'Attrition'])\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Scatter Plot - Salary vs Performance\n",
        "ax3 = axes[1, 0]\n",
        "for dept in df['Department'].unique():\n",
        "    dept_data = df[df['Department'] == dept]\n",
        "    ax3.scatter(dept_data['Salary'], dept_data['PerformanceScore'],\n",
        "               label=dept, alpha=0.6, s=100)\n",
        "ax3.set_title('Salary vs Performance Score by Department', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Salary')\n",
        "ax3.set_ylabel('Performance Score')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Model Performance Comparison\n",
        "ax4 = axes[1, 1]\n",
        "models = ['Random Forest\\n(Attrition)', 'Linear Regression\\n(Performance)', 'Deep Learning\\n(Performance)', 'Deep Learning\\n(Attrition)']\n",
        "scores = [accuracy, r2, r2_dl, test_results[1]]\n",
        "colors = ['#3498db', '#2ecc71', '#e67e22', '#9b59b6']\n",
        "ax4.bar(models, scores, color=colors, alpha=0.7)\n",
        "ax4.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('Score (Accuracy/R²)')\n",
        "ax4.set_ylim([0, 1])\n",
        "for i, v in enumerate(scores):\n",
        "    ax4.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\n✅ ALL PHASES COMPLETED SUCCESSFULLY!\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nPROJECT SUMMARY:\")\n",
        "print(f\"  • Total Employees Analyzed: {len(df)}\")\n",
        "print(f\"  • Features Examined: Age, Salary, Years at Company, Performance, Department\")\n",
        "print(f\"  • Attrition Rate: {prob_attrition*100:.2f}%\")\n",
        "print(f\"  • Best Attrition Model: Random Forest (Accuracy: {accuracy:.2%})\")\n",
        "print(f\"  • Best Performance Model: Deep Learning (R²: {r2_dl:.4f})\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "js5oqRjTriWz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}